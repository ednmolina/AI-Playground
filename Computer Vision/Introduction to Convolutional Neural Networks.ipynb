{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Covnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major difference between a densely connected layer and a convolution layers is that dense layers can learn global patters in their input feature space (patterns involving all the pixels), whereas a convolutional layer can learn local patterns (patterns that don't involve all the pixels).\n",
    "\n",
    "This characteristic gives covnets two interesting properties\n",
    "* *The patters they learn are translation invarient* For example, after learning a certain pattern is in the lower-right corner of an iamge, if in another image that pattern is in the upper-left corder a covnet does not have to learn the new patter. Covnets are data efficient when processig image.\n",
    "\n",
    "* *Covnets can learn spatial heiarchies of patterns* For example the frist layer can learn local patterns like edges, and the second layer can learn larger patterns made of the features from the first layers, and so on. Again, this this makes covnets eficient.\n",
    "\n",
    "Convolutions operate over 3D tensors called *feature maps*, with two spatial exes (*height* and *width*) as well as depth called the *channel axis* The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an *output feature map*. This output feature map is a 3D tensor: it has a width and height. It's depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth no longer represent convential channels like RBG input; rather, they stand for filters. Filters encode specific aspects of the input data.\n",
    "\n",
    "A convolution works by sliding a window over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surface surrounfing features (shape (*window_height*, *window_width*, *input_depth*)). Each 3D patch is then transformed into a tensor of 1D (vector) of shape (*output_depth*,) All these 1D vectors are reassembled into a 3D output map of shape (*height*, *width*, *output_depth*). Every location on the output feature map corresponds to the same location in the input feature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the Network\n",
    "A convolutional neural network or convnet takes as input tensors of shape (image_height, image_width, image_channels). This does not include the batch dimention or the number of images we have.\n",
    "\n",
    "In this case, since we are working with the MNIST dataset, the images are of size (28, 28, 1). Therefore we will be passing in the argument `input_shape=(28, 28, 1)` to the first layer\n",
    "\n",
    "The output of very `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape (height, width, channels). The width and height dimensions is controlling by the first argument passed to the `Conv2D` layers is 32 or 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Conv layer outputs 32 filter of size 3x3 \n",
    "\n",
    "Second Conv layer outputs 64 filter of size 3x3 \n",
    "\n",
    "Third Conv layer outputs 63 filter of size 3x3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(63, (3, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 63)          36351     \n",
      "=================================================================\n",
      "Total params: 55,167\n",
      "Trainable params: 55,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to feed the last output tensor from the network above that has shape (3, 3, 64) into a densely connected classifier network into a stack of `Densse` layers. These classifiers processes 1D vectors whereas the current output is a 3D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are working with the MNIST dataset, this will be a 10-way classification. From the modelsummary we can see that the (3, 3, 64) outputs are flattened into vectors of shape 3*3*64 = (576,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 63)          36351     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 567)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36352     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 92,169\n",
      "Trainable params: 92,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# One hot encode the data\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Covnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 21s 346us/step - loss: 0.1783 - acc: 0.9433\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0491 - acc: 0.9848\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.0331 - acc: 0.9894\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.0244 - acc: 0.9928\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 23s 389us/step - loss: 0.0197 - acc: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x109dbd390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 132us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model reaches a test accuracy of 98.85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
