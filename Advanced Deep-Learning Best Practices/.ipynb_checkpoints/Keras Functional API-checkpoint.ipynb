{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "This notebook will epxplore the more advanced tools availabe for developing state-of-the art models on difficult problems. The functional API in keras allows developers to build graph-like models, share a layer across different inputs, and use Keras models like Python functions. Keras callbacks and TensorBoard allows a developer to monitor models *during* training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API: Going beyond the Sequential Model\n",
    "Until now, the neural networks built prior to this notebook have been implemented using the `Sequential` model. The `Sequential` model assumes that the netorks exactly has one input and exactly one output, and it consists of a linear stack of layers. This model works in many ases, but is too inflexible for some number of cases. Some networks will require multiple inputs, some may require multiple outputs, and some networks have internal branching between layers that make them look like graphs rather than linear stacks of layers.\n",
    "\n",
    "Other tasks require multimodal inputs, where they merge data from different input sources, processing each type of data using different types of layers. For example inputs for predicting a stock price could be text for sentiment analysis, and prices for regression.\n",
    "\n",
    "Some taks need to predict multiple target attributes of input data, meaning they need to have multiple outputs. For exmaple, given a text from a novel is it possble to categorize it into a genre and time period it was written in? It's possble to train it with two models, but these attributes are not statistically independent, a better model can wok by jointly predicting both at the same time. This is because there are correlations with date and genre. For example, William Shakespeare's plays are very much different those of Arthur Miller. Two different playwrites with differing generes and time periods.\n",
    "\n",
    "Futhermore, recently developed neural achitectures require nonlinear network topology: networks that are structed as directed acyclic. graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make three layers\n",
    "A `Dense` layer with 32 units taking in parameters of shape 64\n",
    "\n",
    "Another `Dense` layer with 32 units\n",
    "\n",
    "Last layer is a `Dense` layer outputting a vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation = 'relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation = 'relu'))\n",
    "seq_model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because the `output_tensor` was obtained by repeatedly transforming `input_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate Dummy Data\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 377us/step - loss: 11.6169\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5743\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5652\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5590\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5547\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 11.5504\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5473\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.5450\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5431\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1300c1ba8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 109us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input Models\n",
    "The Keras functional APU can be used to build models that have multiple inputs. These models at some point merge their different input branches using a layer that can combine several tensors by: adding them, concatenating them, etc. This can be done with operations like `keras.layers.add`, `keras.layers.concatenate`, etc. A simple multi-input model is a question-answering model.\n",
    "\n",
    "A typical question-answering model has two inputs\" a natural-language question and a text snippet providing informatioed that will be used for answering the question. The model must produce an answer, the simplest one would be a one word answer obtained from a softmax over some preefined vocabulary.\n",
    "\n",
    "The example below will show how to build a question-answering network with the functional API. Two independent branches will be set up, one that encodes text input and the question input as representation vectors, then concatenare these vectors, and finally add a softmax classifier on the top of the concatenated representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "#from keras import Input\n",
    "from keras.layers import LSTM, Input, Dense, Embedding\n",
    "\n",
    "text_vocabulary_size = int(1e4)\n",
    "question_vocabulary_size = int(10e4)\n",
    "answer_vocabulary_size = int(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Text Input\"\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "\n",
    "# Embeds the inputs into vectors of size 64\n",
    "embedded_text = layers.Embedding(text_vocabulary_size,64)(text_input)\n",
    "\n",
    "# Encodes the vectors in a single vector via an LSTM\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "\"Question Input\"\n",
    "question_input = Input(shape=(None,),dtype='int32',name='question')\n",
    "\n",
    "# Embeds the question into vectors of size 32\n",
    "embedded_question = layers.Embedding(question_vocabulary_size,32)(question_input)\n",
    "\n",
    "# Encode the question into vector of size 16 via an LSTM\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "\"Concatenate the layers\"\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question],axis=-1)\n",
    "\n",
    "# Add a dense layer to output one of 500 words in the dictionary\n",
    "# The dictionary of words is concatenated\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# Specity the two inputs, and then the output\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data to multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of samples and the maximum length of each sample\n",
    "num_samples = int(1e3)\n",
    "max_length = int(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Text Data:  (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Creates a 2D array of 1000 samples\n",
    "# Each value is between 1 and text_vocabulary_size (1e4)\n",
    "text = np.random.randint(1, text_vocabulary_size,size=(num_samples, max_length))\n",
    "print ('Shape of Text Data: ', np.shape(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Question Data:  (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "question = np.random.randint(1, question_vocabulary_size,size=(num_samples, max_length)) \n",
    "print ('Shape of Question Data: ', np.shape(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Answer Data:  (1000, 500)\n"
     ]
    }
   ],
   "source": [
    "# This is an example where answers are one-hot-encoded\n",
    "answers = np.random.randint(0, 2, size=(num_samples, answer_vocabulary_size))\n",
    "\n",
    "print ('Shape of Answer Data: ', np.shape(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data to the model\n",
    "Two ways to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1554.1469 - acc: 1.0000e-03\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1554.0022 - acc: 0.0050\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1553.8422 - acc: 0.0060\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 954us/step - loss: 1553.5595 - acc: 0.0470\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1553.2026 - acc: 0.0200\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 923us/step - loss: 1552.7317 - acc: 0.0200\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 1552.2642 - acc: 0.0070\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 1551.8204 - acc: 0.0120\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 929us/step - loss: 1551.4211 - acc: 1.0000e-03\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 1551.0688 - acc: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Fitting using a list of inputs\n",
    "history = model.fit([text, question], answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1550.7406 - acc: 0.0060\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1550.2596 - acc: 0.0040\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1549.7407 - acc: 0.0090\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1549.2393 - acc: 0.0050\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1548.7335 - acc: 0.0080\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1548.2099 - acc: 0.0050\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1547.7050 - acc: 0.0040\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1547.1992 - acc: 0.0020\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1546.6769 - acc: 0.0040\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1546.1348 - acc: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1324db048>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting using a dictionary of inputs\n",
    "model.fit({'text': text, 'question': question}, answers,\n",
    "                  epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12439f7b8>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure()\n",
    "plt.plot(history.history['loss'], label = 'Loss')\n",
    "plt.plot(history.history['acc'], label = 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output Models\n",
    "The Keras Fuctional API can be used to build models with multiple outputs (or multiple *heads*). An example would be a network that simultaneously predicts different properties of the data such as a network that inputs a series of social media posts from a person and predicts attributs such as age, gender, and income level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready!\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import Input \n",
    "from keras.models import Model \n",
    "\n",
    "vocabulary_size = 50000 \n",
    "num_income_groups = 10 \n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "# embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input) \n",
    "embedded_posts = layers.Embedding(vocabulary_size,256)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu', padding='same')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu', padding='same')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu', padding='same')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu', padding='same')(x) \n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x) \n",
    "\n",
    "# Note that the output layers are given names.\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax',name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "model = Model(posts_input,[age_prediction, income_prediction, gender_prediction])\n",
    "\n",
    "print(\"Model is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, .2, .5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e76d20459730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mincome_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_income_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mincome_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincome_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_income_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgender_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = int(1e3)\n",
    "max_length = int(1e2)\n",
    "\n",
    "posts = np.random.randint(1, vocabulary_size, size=(num_samples, max_length))\n",
    "\n",
    "age_targets = np.random.randint(0, 100, size=(num_samples,1))\n",
    "\n",
    "income_targets = np.random.randint(1, num_income_groups, size=(num_samples,1))\n",
    "income_targets = keras.utils.to_categorical(income_targets,num_income_groups)\n",
    "\n",
    "gender_targets = np.random.randint(0, 2, size=(num_samples,1))\n",
    "\n",
    "np.shape(posts), np.shape(age_targets), np.shape(income_targets), np.shape(gender_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['age_loss'], label = 'Age Loss')\n",
    "plt.plot(history.history['income_loss'], label = 'Income Loss')\n",
    "plt.plot(history.history['gender_loss'], label = 'Gender Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title(\"Loss over Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Acyclic Graphs of Layers\n",
    "With the functional API we can also build models with complex internal topology. Neural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers. It is important they are acyclic because these graphs can't have cycles. It is impossible for a tensor x to be the input of one of the layers generated by x. The only processing loops that are allowed are those internal to recurrent layers.\n",
    "\n",
    "Common neural network components are implemented as graphs. Two otable ones are Inception modules and residual connections.\n",
    "\n",
    "## Inception Models\n",
    "Inception is a populat type of network architecture for convolutional neural networks. An Inception netural network architecture is a stack of modules that themselves look like small independednt networks that are split into several parallel branches. The most basic form of an Inception module has three to four branches starting with a 1x1 convolution, followed by a 3x3 convolution, and ends with the concatenation of the resulting features from the previous layers. __The advantage of this type of network is the ability of the network to separately learn spatial features and channel-wise features. This is more efficient that learning them jointly.__ More complex forms of the Inception module are possible which involve pooling operation, varying spatial convolution sizes, and branches without spatial convolution.\n",
    "\n",
    "The Inception model is available as `keras.applications.inception_v3.InceptionV3` which includes pretrained weights on the ImageNet dataset.\n",
    "\n",
    "Another  popular model in Keras is *Xception* which stands for *extreme inception* which is a covnet architecture loosely inspired by the *Inception* model. It replaces Inception modules with depth-wise seperable seperable convolutions consisting of a depthwise convolution (a spatial convolution where every input channel is handeled separately) followed by a pointwise convolution (1x1 convolution). This is an extreme form of the Indeption module where spatial features an channel-wise features are __fully__ separated.The advantage of the *Xception* model is it's better runtime performance and higher accuracy on ImageNet as well as other large-scale datasets. This is due to a more efficient use of model parameters.\n",
    "\n",
    "## 1x1 convolution\n",
    "A 1x1 convolution extracts spatial patches around every tile in an input tensor and applies the same transformation to every patch in the input tensor. This operation is equivalent to running each tile vector through a *Dense* layer. The network will compute features that mix together information from the other channels of the input tensor; however, it won't mix information across space as we are looking at one tile at a time. This type of convolution is called *pointwise convolution* which are featured in Inception modules where they contribite to factoring out channel-wise feature leanring and space wise feature leanring where we assume that each hannel is highly autocorrelated across space, but different channels may not be highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# Every branch in the network will have the same stride value (2)\n",
    "# This is necessary to keep all the branch outputs the same size so they can be concatenated\n",
    "branch_a = layers.Conv2D(128, 1,\n",
    "                         activation = 'relu',\n",
    "                         strides = 2)(x)\n",
    "# The striding occurs in the same spatial convoltuion layers\n",
    "branch_b = layers.Conv2D(128, 1, activation = 'relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation = 'relu', strides = 2)(branch_b)\n",
    "\n",
    "# In this branch, the striding, or sliding occurs in the average pooling layer\n",
    "branch_c = layers.AveragePooling2D(3, strids = 2)(x)\n",
    "branch_c = layers.Conve2D(128, 3, activation = 'relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation = 'relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation = 'relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation = 'relu', strides = 2)(branch_d)\n",
    "\n",
    "# This branch concatenates the branch outputs to ontain the module output\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Connections\n",
    "*Residual connections* are a common graph-like network component found in many network architectures after 2015. This architecture tackles two common problems that plague large-scale deep-learning models: canishing gradients and representational bottlenecks. Adding residual connection to anymodel that has more than 10 layers is likely to be beneficial.\n",
    "\n",
    "A residual connection consists of marking the output of an earlier layer available as input to a later layer. THis creates a shortcut in a sequential network between the layers. Instead of concatenating layers to the later activation, the earlier output is summed with the later activation-on the assumption that both activations are the same size. If the outputs are different sizes, a linear transform to reshape the earlier activation into the target shape. A *Dense* layer without an activation, for for convolutional feature maps, a 1x1 convolution without an activation can accomplish this.\n",
    "\n",
    "Below is how to implement residual connections in Keras when the feature-map sizse are the same, using identity residual connections. The model below assumes the existance of 4D tensor inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = ...\n",
    "y = layers.Conv2D(138, 3, activation = 'relu', padding = 'same')(x)\n",
    "y = layers.Conv2D(128, 3, activation = 'relu', padding = 'same')(y)\n",
    "y = layers.Conv2D(128, 3, activation = 'relu', padding = 'same')(y)\n",
    "\n",
    "# This is the residual connection when the feature-map sizes are different\n",
    "# It uses a linear residual connection that assume a 4D input tensor\n",
    "residual = layers.Conv2D(128, 1, strides = 2, padding = 'same')(x)\n",
    "\n",
    "y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational Bottlenecks in Deep Learning\n",
    "In a Sequential model, each representation layer is built on top of the previous one. This means that it has acess to information contained in the activations of the previous layer. If one layer is too small, then the model will be constrained by how much information can be crammed in to the activations of this layer.\n",
    "\n",
    "### Vanishing Gradients in Deep Learning\n",
    "Backpropagation works by propagating a feedback signal from the output loss down to the earlier layers. If this feed-back signal has to be propagated through a deep stack of layers, the signal may be tenuous or even lost entirely, which renders the network retrainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the functional API, models can be used as layers. Each model can be thought of a larger layers. A model on an input tensor can retrieve an output tensor:\n",
    "```python\n",
    "y = model(x)\n",
    "```\n",
    "\n",
    "If the model contains multiple input tensors and multiple output tensor then it should be called with a list of tensors\n",
    "```python\n",
    "y1, y2 = model([x1, x2])\n",
    "```\n",
    "\n",
    "When a model instance is called, the weights of the model are being reused, similar to what happens when a layer instance is called. An application of this would be when extracting visual features from two closely spaced cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_base = applications.Xception(weights = None, include_top = False)\n",
    "left_input = Input(shape = (250, 250, 3))\n",
    "right_input = Input(shape = (250, 250, 3))\n",
    "\n",
    "left_features = xception_base(left_input)\n",
    "right_features = xception_base(left_input)\n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_input], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
