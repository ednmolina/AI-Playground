{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting and Monitoring Deep-Learning Models Using Keras Callbacks and TensorBoard\n",
    "Until now there was very little contron of running deep learning models after using `model.fit()` or `model.fit_generator`.\n",
    "\n",
    "## Using callbacks to act on a model during training\n",
    "When training a model it is difficult to predict how many epochs will be needed to obtain an optimal validation or loss. A good way to determine when to stop training is when it is measured that the validation loss is no longer improving. A *callback* is an object that is passed to the model in the call to *fit* and that is called by the model at various points during training. A callback object has access to all the available data about the state of the model and its performance, and it can take action:\n",
    "    * interrupt training\n",
    "    * save a model\n",
    "    * load a different weight set\n",
    "    * alter the state of he model\n",
    "    \n",
    "Some examples of using callbacks are:\n",
    "    * Model checkpointing-saving the current weights of the model at different points during training (keras.callbacks.ModelCheckpoint)\n",
    "    * Early stopping-interrupting training when the validation loss is no longer imrpoving (keras.callbacks.EarlyStopping)\n",
    "    * Dynamically adjusting the value of certain parameters during training-such as adjusting the learning rate of the optimizer (keras.callbacks.LearningRateScheduler, (keras.callbacks.ReduceLROnPlateu, keras.callbacks.CSVLogger)\n",
    "    * Logging training and validation metrics during training or visualizing the representations learned by the model as they're updated-For example, the keras progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint and EarlyStopping Callbacks\n",
    "The Early stopping callback can be used to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. This allows one to interrupt training as soon as overfitting occurs, this aviding the need to retrain the model for a smaller number of epochs. This is usually used with `ModelCheckpoint` which lets you continiously save the mode during training.\n",
    "\n",
    "```python\n",
    "import keras\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience = 1,\n",
    "),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'my_model.h5',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    "    )\n",
    "]\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy'.\n",
    "              metrics = ['acc'])\n",
    "model.fit(x, y,\n",
    "          epochs = 10,\n",
    "          batch_size = 32,\n",
    "          callbacks = callbacks_list,\n",
    "          validation_data = (x_val, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau\n",
    "This callback can be used to reduce the learning rate when validation loss has stopped improving. Reducing or increasing the learning rate in case of a *loss plateau* is an effective strategy to escape a local minima during training.\n",
    "\n",
    "```python\n",
    "callbacks_list = [keras.callbacks.ReduceLROnPlateau(minitor = 'val_loss',\n",
    "                                                    factor = 0.1,\n",
    "                                                    patience = 10)]\n",
    "model.fit(x, y,\n",
    "          epochs = 10,\n",
    "          batch_size = 32,\n",
    "          callbacks = callbacks_list,\n",
    "          validation_data = (x_val, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Callback\n",
    "Callbacks are implemented by subclassing the class `keras.callbacks.Callback`. They can be implemented at any number of the following transparantly named mathods, which are called at vairous parts of the training:\n",
    "```python\n",
    "# Start|End of every epoch\n",
    "on_epoch_begin\n",
    "on_epoch_ebd\n",
    "\n",
    "# Start|End of every batch\n",
    "on_batch_begin\n",
    "on_batch_end\n",
    "\n",
    "# Start|End of training\n",
    "on_train_begin\n",
    "on_train_end\n",
    "```\n",
    "These methods are called with a `logs` argument, which essentially is a dictionary containing information about the previous batch, epoch, or training run: training and validation metrics, and so on.\n",
    "\n",
    "The callback object also has acess to the attributes:\n",
    "    * `self.model` the model instance from which the callback is being called\n",
    "    * `self.validation_data` the value of what was passed to `fit` as validation data\n",
    "    \n",
    "Here is an example of a custom callback that saves to disk, Numpy arrays, the activation of every layer of the model at the end of every epoch, computed on the first sample of the validation set:\n",
    "\n",
    "```python\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    \n",
    "    # Called by the parent model before training, to inform the callback object what model is calling it\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        # Model isntance that returns the activations of all the layers\n",
    "        self.activation_model = keras.models.Model(model.input,\n",
    "                                                  layer_outputs)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data')\n",
    "           \n",
    "        # gets the first input sample of the validation data\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        \n",
    "        f = open('Activation_at_epoch' + str(epoch)+'.npz', 'w')\n",
    "        np.savez(f, activations)\n",
    "        f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
